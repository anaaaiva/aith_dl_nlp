{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Домашнее задание 1 - 10 баллов\n",
    "\n",
    "1. Загрузите набор данных lenta-ru-news с помощью библиотеки Corus для задачи классификации текстов по топикам (пригодятся атрибуты title, text, topic)- **1 балл**\n",
    "2. Подготовьте данные к обучению: - **3 балла**\n",
    "    - Предобработайте данные: реализуйте оптимальную, на ваш взгляд, предобработку текстов (нормализация, очистка, стемминг/лемматизация и т.п.) и таргета.\n",
    "    - **hint**: для ускорения обработки  и обучения можно ограничиться не всем датасетом, а его репрезентативной частью, например, размера 100_000.\n",
    "    - Кратко опишите пайплайн, на котором остановились, и почему.\n",
    "    - Разделите датасет на обучающую, валидационную и тестовую выборки со стратификацией в пропорции 60/20/20. В качестве целевой переменной используйте атрибут `topic`\n",
    "3. Замерьте базовое качество с любым dummy-бейзлайном - **0.5 балла**\n",
    "4. Обучите модель `sklearn.linear_model.LogisticRegression` с двумя вариантами векторизации: **2 балла**\n",
    "  - `sklearn.feature_extraction.text.CountVectorizer`\n",
    "  - `sklearn.feature_extraction.text.TfidfVectorizer`\n",
    "5. Попробуйте улучшить качество, подобрав оптимальные гиперпараметры трансформаций и модели на кросс-валидации **1 балл**\n",
    "6. Оцените качество лучшего пайплайна на отложенной выборке - **0.5 балла**\n",
    "\n",
    "**Общее**\n",
    "\n",
    "- Принимаемые решения обоснованы (почему выбрана определенная архитектура/гиперпараметр/оптимизатор/преобразование и т.п.) - **1 балл**\n",
    "- Обеспечена воспроизводимость решения: зафиксированы random_state, ноутбук воспроизводится от начала до конца без ошибок - **1 балл**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ivawi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\ivawi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\ivawi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import itertools\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from pymystem3 import Mystem\n",
    "import warnings\n",
    "from tqdm import tqdm\n",
    "from joblib import Parallel, delayed\n",
    "import optuna\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "tqdm.pandas()\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Загрузка данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !curl -L -o data/lenta-ru-news.csv.gz https://github.com/yutkin/Lenta.Ru-News-Dataset/releases/download/v1.0/lenta-ru-news.csv.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>topic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Названы регионы России с самой высокой смертно...</td>\n",
       "      <td>Вице-премьер по социальным вопросам Татьяна Го...</td>\n",
       "      <td>Россия</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Австрия не представила доказательств вины росс...</td>\n",
       "      <td>Австрийские правоохранительные органы не предс...</td>\n",
       "      <td>Спорт</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Обнаружено самое счастливое место на планете</td>\n",
       "      <td>Сотрудники социальной сети Instagram проанализ...</td>\n",
       "      <td>Путешествия</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>В США раскрыли сумму расходов на расследование...</td>\n",
       "      <td>С начала расследования российского вмешательст...</td>\n",
       "      <td>Мир</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Хакеры рассказали о планах Великобритании зами...</td>\n",
       "      <td>Хакерская группировка Anonymous опубликовала н...</td>\n",
       "      <td>Мир</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0  Названы регионы России с самой высокой смертно...   \n",
       "1  Австрия не представила доказательств вины росс...   \n",
       "2       Обнаружено самое счастливое место на планете   \n",
       "3  В США раскрыли сумму расходов на расследование...   \n",
       "4  Хакеры рассказали о планах Великобритании зами...   \n",
       "\n",
       "                                                text        topic  \n",
       "0  Вице-премьер по социальным вопросам Татьяна Го...       Россия  \n",
       "1  Австрийские правоохранительные органы не предс...        Спорт  \n",
       "2  Сотрудники социальной сети Instagram проанализ...  Путешествия  \n",
       "3  С начала расследования российского вмешательст...          Мир  \n",
       "4  Хакерская группировка Anonymous опубликовала н...          Мир  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "limit = 100000\n",
    "\n",
    "# не увидела смысла использовать load_lenta из corus - все равно потом складывать все в pd.DataFrame,\n",
    "# к тому же прямая загрузка быстрее работала\n",
    "# records = load_lenta('data/lenta-ru-news.csv.gz')\n",
    "records = pd.read_csv('data/lenta-ru-news.csv.gz', compression='gzip')\n",
    "df = pd.DataFrame(records)[['title', 'text', 'topic']][:limit]\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "topic\n",
       "Россия               15151\n",
       "Мир                  14421\n",
       "Спорт                10045\n",
       "Экономика             7682\n",
       "Интернет и СМИ        6935\n",
       "Силовые структуры     6925\n",
       "Бывший СССР           6810\n",
       "Культура              6578\n",
       "Наука и техника       5645\n",
       "Из жизни              4903\n",
       "Ценности              4480\n",
       "Дом                   3408\n",
       "Путешествия           3223\n",
       "Бизнес                1993\n",
       "69-я параллель         815\n",
       "Крым                   661\n",
       "Культпросвет           307\n",
       "Оружие                   1\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.topic.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Подготовка данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "STOP_RU = list(stopwords.words('russian'))\n",
    "\n",
    "\n",
    "def preprocess_text(text, stopwords=STOP_RU):\n",
    "    tokens = re.sub(r'[^а-яё]', ' ', text.lower()).split()\n",
    "    tokens = [word for word in tokens if word not in stopwords]\n",
    "    # lemmatized = [mystem.lemmatize(word)[0] for word in tokens]\n",
    "    return ' '.join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100000/100000 [00:02<00:00, 41028.48it/s]\n"
     ]
    }
   ],
   "source": [
    "df['processed_text'] = Parallel(n_jobs=-1)(delayed(preprocess_text)(t) for t in tqdm(df['text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [02:19<00:00,  1.40s/it]\n"
     ]
    }
   ],
   "source": [
    "batch_size = 1000\n",
    "texts = list(df['processed_text'])\n",
    "text_batch = [texts[i : i + batch_size] for i in range(0, len(texts), batch_size)]\n",
    "\n",
    "m = Mystem()\n",
    "\n",
    "\n",
    "def lemmatize(text):\n",
    "    merged_text = '|'.join(text)\n",
    "\n",
    "    doc = []\n",
    "    res = []\n",
    "\n",
    "    for t in m.lemmatize(merged_text):\n",
    "        if t != '|':\n",
    "            doc.append(t)\n",
    "        else:\n",
    "            res.append(''.join(doc))\n",
    "            doc = []\n",
    "    res.append(''.join(doc))\n",
    "    return res\n",
    "\n",
    "\n",
    "processed_texts = list(itertools.chain(*Parallel(n_jobs=-1)(delayed(lemmatize)(t) for t in tqdm(text_batch))))\n",
    "df['lemmatized_text'] = processed_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>topic</th>\n",
       "      <th>processed_text</th>\n",
       "      <th>lemmatized_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Названы регионы России с самой высокой смертно...</td>\n",
       "      <td>Вице-премьер по социальным вопросам Татьяна Го...</td>\n",
       "      <td>Россия</td>\n",
       "      <td>вице премьер социальным вопросам татьяна голик...</td>\n",
       "      <td>вица премьер социальный вопрос татьяна голиков...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Австрия не представила доказательств вины росс...</td>\n",
       "      <td>Австрийские правоохранительные органы не предс...</td>\n",
       "      <td>Спорт</td>\n",
       "      <td>австрийские правоохранительные органы представ...</td>\n",
       "      <td>австрийский правоохранительный орган представл...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Обнаружено самое счастливое место на планете</td>\n",
       "      <td>Сотрудники социальной сети Instagram проанализ...</td>\n",
       "      <td>Путешествия</td>\n",
       "      <td>сотрудники социальной сети проанализировали по...</td>\n",
       "      <td>сотрудник социальный сеть проанализировать пос...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>В США раскрыли сумму расходов на расследование...</td>\n",
       "      <td>С начала расследования российского вмешательст...</td>\n",
       "      <td>Мир</td>\n",
       "      <td>начала расследования российского вмешательства...</td>\n",
       "      <td>начинать расследование российский вмешательств...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Хакеры рассказали о планах Великобритании зами...</td>\n",
       "      <td>Хакерская группировка Anonymous опубликовала н...</td>\n",
       "      <td>Мир</td>\n",
       "      <td>хакерская группировка опубликовала новые докум...</td>\n",
       "      <td>хакерский группировка опубликовывать новый док...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0  Названы регионы России с самой высокой смертно...   \n",
       "1  Австрия не представила доказательств вины росс...   \n",
       "2       Обнаружено самое счастливое место на планете   \n",
       "3  В США раскрыли сумму расходов на расследование...   \n",
       "4  Хакеры рассказали о планах Великобритании зами...   \n",
       "\n",
       "                                                text        topic  \\\n",
       "0  Вице-премьер по социальным вопросам Татьяна Го...       Россия   \n",
       "1  Австрийские правоохранительные органы не предс...        Спорт   \n",
       "2  Сотрудники социальной сети Instagram проанализ...  Путешествия   \n",
       "3  С начала расследования российского вмешательст...          Мир   \n",
       "4  Хакерская группировка Anonymous опубликовала н...          Мир   \n",
       "\n",
       "                                      processed_text  \\\n",
       "0  вице премьер социальным вопросам татьяна голик...   \n",
       "1  австрийские правоохранительные органы представ...   \n",
       "2  сотрудники социальной сети проанализировали по...   \n",
       "3  начала расследования российского вмешательства...   \n",
       "4  хакерская группировка опубликовала новые докум...   \n",
       "\n",
       "                                     lemmatized_text  \n",
       "0  вица премьер социальный вопрос татьяна голиков...  \n",
       "1  австрийский правоохранительный орган представл...  \n",
       "2  сотрудник социальный сеть проанализировать пос...  \n",
       "3  начинать расследование российский вмешательств...  \n",
       "4  хакерский группировка опубликовывать новый док...  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('data/clean_lenta-ru-news.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Остановилась на самом классическом варианте: привела к нижнему регистру, убрала лишние символы, токенизировала по словам, далее использовала лемматизацию. Остановилась на ней, так как у нас нет ограничения на время обработки датасета и можно распараллелить, если не устраивает скорость. Лемматизация просто более сложная процедура чем стемминг и теряет меньше смыслов в предложении."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# у меня получилось так, что topic Оружие встретился всего 1 раз, поэтому пришлось его удалить из-за невозможности стратификации\n",
    "df = df[df['topic'] != 'Оружие']\n",
    "\n",
    "df = df.dropna()\n",
    "X = df['lemmatized_text']\n",
    "y = df['topic']\n",
    "\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.4, stratify=y, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, stratify=y_temp, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((59989,), (19996,), (19997,))"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, X_val.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Dummy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dummy accuracy: 0.1515\n"
     ]
    }
   ],
   "source": [
    "dummy = DummyClassifier(strategy='most_frequent')\n",
    "dummy.fit(X_train, y_train)\n",
    "dummy_pred = dummy.predict(X_val)\n",
    "print(f'Dummy accuracy: {accuracy_score(y_val, dummy_pred):.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CountVectorizer accuracy: 0.8380\n",
      "TfidfVectorizer accuracy: 0.8342\n"
     ]
    }
   ],
   "source": [
    "vectorizers = {'CountVectorizer': CountVectorizer(), 'TfidfVectorizer': TfidfVectorizer()}\n",
    "\n",
    "for name, vectorizer in vectorizers.items():\n",
    "    pipeline = Pipeline(\n",
    "        [('vectorizer', vectorizer), ('classifier', LogisticRegression(max_iter=1000, random_state=42))]\n",
    "    )\n",
    "\n",
    "    pipeline.fit(X_train, y_train)\n",
    "    val_pred = pipeline.predict(X_val)\n",
    "    accuracy = accuracy_score(y_val, val_pred)\n",
    "    print(f'{name} accuracy: {accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. (Op)Tuning :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-25 19:33:23,671] A new study created in memory with name: no-name-3a6f2cfe-6ffe-49c0-86b0-199ab8f418b6\n",
      "[I 2025-02-25 19:34:03,092] Trial 0 finished with value: 0.8213839979970544 and parameters: {'vectorizer': 'TfidfVectorizer', 'max_df': 0.8351948636043404, 'min_df': 15, 'C': 1.2238156722163405, 'max_iter': 746, 'solver': 'lbfgs', 'penalty': None, 'use_scaler': False}. Best is trial 0 with value: 0.8213839979970544.\n",
      "[I 2025-02-25 19:34:12,919] Trial 1 finished with value: 0.8172331867208621 and parameters: {'vectorizer': 'TfidfVectorizer', 'max_df': 0.9077070665925416, 'min_df': 9, 'C': 1.0609593278868934, 'max_iter': 785, 'solver': 'lbfgs', 'penalty': 'l2', 'use_scaler': True}. Best is trial 0 with value: 0.8213839979970544.\n",
      "[I 2025-02-25 19:34:12,921] Trial 2 finished with value: -inf and parameters: {'vectorizer': 'TfidfVectorizer', 'max_df': 0.9226728970836465, 'min_df': 9, 'C': 0.04356038495172463, 'max_iter': 712, 'solver': 'liblinear', 'penalty': None, 'use_scaler': False}. Best is trial 0 with value: 0.8213839979970544.\n",
      "[I 2025-02-25 19:34:37,463] Trial 3 finished with value: 0.8207338660247103 and parameters: {'vectorizer': 'TfidfVectorizer', 'max_df': 0.8937069406026241, 'min_df': 15, 'C': 1.0244592201863933, 'max_iter': 986, 'solver': 'liblinear', 'penalty': 'l1', 'use_scaler': False}. Best is trial 0 with value: 0.8213839979970544.\n",
      "[I 2025-02-25 19:34:37,466] Trial 4 finished with value: -inf and parameters: {'vectorizer': 'CountVectorizer', 'max_df': 0.9248955310085875, 'min_df': 11, 'C': 0.4646118710928367, 'max_iter': 329, 'solver': 'liblinear', 'penalty': None, 'use_scaler': True}. Best is trial 0 with value: 0.8213839979970544.\n",
      "[I 2025-02-25 19:37:13,267] Trial 5 finished with value: 0.8278684704237126 and parameters: {'vectorizer': 'CountVectorizer', 'max_df': 0.9293060440599894, 'min_df': 10, 'C': 1.3396603251615857, 'max_iter': 512, 'solver': 'liblinear', 'penalty': 'l2', 'use_scaler': False}. Best is trial 5 with value: 0.8278684704237126.\n",
      "[I 2025-02-25 19:38:11,626] Trial 6 finished with value: 0.8327360261468935 and parameters: {'vectorizer': 'CountVectorizer', 'max_df': 0.9732939343051693, 'min_df': 3, 'C': 0.01390826792990606, 'max_iter': 949, 'solver': 'liblinear', 'penalty': 'l2', 'use_scaler': False}. Best is trial 6 with value: 0.8327360261468935.\n",
      "[I 2025-02-25 19:38:22,258] Trial 7 finished with value: 0.8192668214839453 and parameters: {'vectorizer': 'TfidfVectorizer', 'max_df': 0.9802523765799345, 'min_df': 8, 'C': 0.43126792526232394, 'max_iter': 785, 'solver': 'lbfgs', 'penalty': 'l2', 'use_scaler': True}. Best is trial 6 with value: 0.8327360261468935.\n",
      "[I 2025-02-25 19:38:22,260] Trial 8 finished with value: -inf and parameters: {'vectorizer': 'TfidfVectorizer', 'max_df': 0.9439826078854189, 'min_df': 10, 'C': 0.02640069638444502, 'max_iter': 598, 'solver': 'liblinear', 'penalty': None, 'use_scaler': True}. Best is trial 6 with value: 0.8327360261468935.\n",
      "[I 2025-02-25 19:38:32,243] Trial 9 finished with value: 0.8137991935334504 and parameters: {'vectorizer': 'CountVectorizer', 'max_df': 0.9234944213983717, 'min_df': 11, 'C': 0.0284522747397442, 'max_iter': 562, 'solver': 'lbfgs', 'penalty': None, 'use_scaler': True}. Best is trial 6 with value: 0.8327360261468935.\n",
      "[I 2025-02-25 19:39:23,192] Trial 10 finished with value: 0.8151828130394826 and parameters: {'vectorizer': 'CountVectorizer', 'max_df': 0.9897376789947796, 'min_df': 2, 'C': 5.278574352262387, 'max_iter': 997, 'solver': 'liblinear', 'penalty': 'l1', 'use_scaler': False}. Best is trial 6 with value: 0.8327360261468935.\n",
      "[I 2025-02-25 19:41:07,706] Trial 11 finished with value: 0.8393872638854829 and parameters: {'vectorizer': 'CountVectorizer', 'max_df': 0.8696111085956567, 'min_df': 3, 'C': 0.12166295197438395, 'max_iter': 435, 'solver': 'liblinear', 'penalty': 'l2', 'use_scaler': False}. Best is trial 11 with value: 0.8393872638854829.\n",
      "[I 2025-02-25 19:42:45,621] Trial 12 finished with value: 0.8402207333519216 and parameters: {'vectorizer': 'CountVectorizer', 'max_df': 0.8558572243919029, 'min_df': 2, 'C': 0.09531024695963544, 'max_iter': 395, 'solver': 'liblinear', 'penalty': 'l2', 'use_scaler': False}. Best is trial 12 with value: 0.8402207333519216.\n",
      "[I 2025-02-25 19:44:22,043] Trial 13 finished with value: 0.8396206166616389 and parameters: {'vectorizer': 'CountVectorizer', 'max_df': 0.855662358482152, 'min_df': 5, 'C': 0.10387713032437666, 'max_iter': 367, 'solver': 'liblinear', 'penalty': 'l2', 'use_scaler': False}. Best is trial 12 with value: 0.8402207333519216.\n",
      "[I 2025-02-25 19:46:04,069] Trial 14 finished with value: 0.8396039638903448 and parameters: {'vectorizer': 'CountVectorizer', 'max_df': 0.800185142742609, 'min_df': 5, 'C': 0.11336029478999814, 'max_iter': 301, 'solver': 'liblinear', 'penalty': 'l2', 'use_scaler': False}. Best is trial 12 with value: 0.8402207333519216.\n",
      "[I 2025-02-25 19:47:43,356] Trial 15 finished with value: 0.8390705235902243 and parameters: {'vectorizer': 'CountVectorizer', 'max_df': 0.8603620131604505, 'min_df': 6, 'C': 0.11404664128977322, 'max_iter': 419, 'solver': 'liblinear', 'penalty': 'l2', 'use_scaler': False}. Best is trial 12 with value: 0.8402207333519216.\n",
      "[I 2025-02-25 19:49:39,762] Trial 16 finished with value: 0.8372535332532915 and parameters: {'vectorizer': 'CountVectorizer', 'max_df': 0.8279759132429012, 'min_df': 5, 'C': 0.20654337596657701, 'max_iter': 403, 'solver': 'liblinear', 'penalty': 'l2', 'use_scaler': False}. Best is trial 12 with value: 0.8402207333519216.\n",
      "[I 2025-02-25 19:50:01,170] Trial 17 finished with value: 0.8087149892157154 and parameters: {'vectorizer': 'CountVectorizer', 'max_df': 0.8759528352726854, 'min_df': 6, 'C': 0.05808098315710597, 'max_iter': 483, 'solver': 'liblinear', 'penalty': 'l1', 'use_scaler': False}. Best is trial 12 with value: 0.8402207333519216.\n",
      "[I 2025-02-25 19:51:13,195] Trial 18 finished with value: 0.8340695956341699 and parameters: {'vectorizer': 'CountVectorizer', 'max_df': 0.8459959298695402, 'min_df': 2, 'C': 0.20016681727932203, 'max_iter': 378, 'solver': 'lbfgs', 'penalty': 'l2', 'use_scaler': False}. Best is trial 12 with value: 0.8402207333519216.\n",
      "[I 2025-02-25 19:52:07,148] Trial 19 finished with value: 0.8284185676635307 and parameters: {'vectorizer': 'CountVectorizer', 'max_df': 0.8119959289876694, 'min_df': 4, 'C': 0.010385425758346781, 'max_iter': 645, 'solver': 'liblinear', 'penalty': 'l2', 'use_scaler': False}. Best is trial 12 with value: 0.8402207333519216.\n",
      "[I 2025-02-25 19:52:55,320] Trial 20 finished with value: 0.812282257373256 and parameters: {'vectorizer': 'CountVectorizer', 'max_df': 0.8949214878007647, 'min_df': 7, 'C': 7.622290636644448, 'max_iter': 491, 'solver': 'liblinear', 'penalty': 'l1', 'use_scaler': False}. Best is trial 12 with value: 0.8402207333519216.\n",
      "[I 2025-02-25 19:54:25,314] Trial 21 finished with value: 0.8397373111127979 and parameters: {'vectorizer': 'CountVectorizer', 'max_df': 0.8051452597466099, 'min_df': 5, 'C': 0.08648320086672628, 'max_iter': 323, 'solver': 'liblinear', 'penalty': 'l2', 'use_scaler': False}. Best is trial 12 with value: 0.8402207333519216.\n",
      "[I 2025-02-25 19:55:48,212] Trial 22 finished with value: 0.8400873833505329 and parameters: {'vectorizer': 'CountVectorizer', 'max_df': 0.8493447294249677, 'min_df': 5, 'C': 0.05981907125264219, 'max_iter': 346, 'solver': 'liblinear', 'penalty': 'l2', 'use_scaler': False}. Best is trial 12 with value: 0.8402207333519216.\n",
      "[I 2025-02-25 19:57:11,451] Trial 23 finished with value: 0.8401040472375693 and parameters: {'vectorizer': 'CountVectorizer', 'max_df': 0.8185911252661366, 'min_df': 4, 'C': 0.056551710925350315, 'max_iter': 303, 'solver': 'liblinear', 'penalty': 'l2', 'use_scaler': False}. Best is trial 12 with value: 0.8402207333519216.\n",
      "[I 2025-02-25 19:58:17,714] Trial 24 finished with value: 0.8371368443600037 and parameters: {'vectorizer': 'CountVectorizer', 'max_df': 0.8222542543328932, 'min_df': 3, 'C': 0.023462231325115674, 'max_iter': 463, 'solver': 'liblinear', 'penalty': 'l2', 'use_scaler': False}. Best is trial 12 with value: 0.8402207333519216.\n",
      "[I 2025-02-25 19:59:38,834] Trial 25 finished with value: 0.8405708000317855 and parameters: {'vectorizer': 'CountVectorizer', 'max_df': 0.8413527045302485, 'min_df': 2, 'C': 0.04900401112747474, 'max_iter': 365, 'solver': 'liblinear', 'penalty': 'l2', 'use_scaler': False}. Best is trial 25 with value: 0.8405708000317855.\n",
      "[I 2025-02-25 20:00:01,290] Trial 26 finished with value: 0.807047975251578 and parameters: {'vectorizer': 'CountVectorizer', 'max_df': 0.8342061609585961, 'min_df': 2, 'C': 0.039726594958326276, 'max_iter': 548, 'solver': 'lbfgs', 'penalty': 'l2', 'use_scaler': True}. Best is trial 25 with value: 0.8405708000317855.\n",
      "[I 2025-02-25 20:02:07,496] Trial 27 finished with value: 0.8370535068617407 and parameters: {'vectorizer': 'CountVectorizer', 'max_df': 0.8714012294567037, 'min_df': 3, 'C': 0.27161902714820685, 'max_iter': 382, 'solver': 'liblinear', 'penalty': 'l2', 'use_scaler': False}. Best is trial 25 with value: 0.8405708000317855.\n",
      "[I 2025-02-25 20:02:26,244] Trial 28 finished with value: 0.7677741461258518 and parameters: {'vectorizer': 'CountVectorizer', 'max_df': 0.8221703651374999, 'min_df': 4, 'C': 0.020136732425421158, 'max_iter': 444, 'solver': 'liblinear', 'penalty': 'l1', 'use_scaler': False}. Best is trial 25 with value: 0.8405708000317855.\n",
      "[I 2025-02-25 20:02:44,064] Trial 29 finished with value: 0.7408691632262403 and parameters: {'vectorizer': 'TfidfVectorizer', 'max_df': 0.8436265649637521, 'min_df': 13, 'C': 0.062419943737226535, 'max_iter': 304, 'solver': 'lbfgs', 'penalty': 'l2', 'use_scaler': False}. Best is trial 25 with value: 0.8405708000317855.\n"
     ]
    }
   ],
   "source": [
    "# использовала optuna, так у нее под капотом метод TPE, который работает быстрее и точнее того же простого grid search\n",
    "# TPE - метод байесовской оптимизации, который на каждом шаге пытается предсказать, какие параметры дадут лучший результат,\n",
    "# строя 2 вероятностные модели. Это к пункту про то, почему были выбраны определенные параметры трансформаций и модели\n",
    "# - потому что они дают лучший результат (хоть и не намного)\n",
    "def objective(trial):\n",
    "    vectorizer_type = trial.suggest_categorical('vectorizer', ['CountVectorizer', 'TfidfVectorizer'])\n",
    "    # ngram_range = trial.suggest_categorical('ngram_range', [(1, 1), (1, 2)]) # по памяти не вывозит\n",
    "    max_df = trial.suggest_float('max_df', 0.8, 0.99)\n",
    "    min_df = trial.suggest_int('min_df', 2, 15)\n",
    "    C = trial.suggest_loguniform('C', 0.01, 10.0)\n",
    "    max_iter = trial.suggest_int('max_iter', 300, 1000)\n",
    "    solver = trial.suggest_categorical('solver', ['liblinear', 'lbfgs'])\n",
    "    penalty = trial.suggest_categorical('penalty', ['l1', 'l2', None])\n",
    "    use_scaler = trial.suggest_categorical('use_scaler', [True, False])\n",
    "\n",
    "    invalid_combinations = {\n",
    "        'liblinear': [None],\n",
    "        'lbfgs': ['l1'],\n",
    "    }\n",
    "\n",
    "    if penalty in invalid_combinations.get(solver, []):\n",
    "        return float('-inf')\n",
    "        # возвращаем плохой результат, чтобы Optuna отбросила эту комбинацию\n",
    "        # тк есть solvers, которые не работают с определенными penalty\n",
    "\n",
    "    if vectorizer_type == 'CountVectorizer':\n",
    "        vectorizer = CountVectorizer(max_df=max_df, min_df=min_df)\n",
    "    else:\n",
    "        vectorizer = TfidfVectorizer(max_df=max_df, min_df=min_df)\n",
    "\n",
    "    if use_scaler:\n",
    "        pipeline = Pipeline(\n",
    "            [\n",
    "                ('vectorizer', vectorizer),\n",
    "                ('scaler', StandardScaler(with_mean=False)),\n",
    "                (\n",
    "                    'classifier',\n",
    "                    LogisticRegression(C=C, random_state=42, max_iter=max_iter, solver=solver, penalty=penalty),\n",
    "                ),\n",
    "            ]\n",
    "        )\n",
    "    else:\n",
    "        pipeline = Pipeline(\n",
    "            [\n",
    "                ('vectorizer', vectorizer),\n",
    "                (\n",
    "                    'classifier',\n",
    "                    LogisticRegression(C=C, random_state=42, max_iter=max_iter, solver=solver, penalty=penalty),\n",
    "                ),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    scores = cross_val_score(pipeline, X_train, y_train, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "    return np.mean(scores)\n",
    "\n",
    "\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'vectorizer': 'CountVectorizer', 'max_df': 0.8413527045302485, 'min_df': 2, 'C': 0.04900401112747474, 'max_iter': 365, 'solver': 'liblinear', 'penalty': 'l2', 'use_scaler': False}\n"
     ]
    }
   ],
   "source": [
    "best_params = study.best_params\n",
    "print(best_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Final accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters: {'vectorizer': 'CountVectorizer', 'max_df': 0.8413527045302485, 'min_df': 2, 'C': 0.04900401112747474, 'max_iter': 365, 'solver': 'liblinear', 'penalty': 'l2', 'use_scaler': False}\n",
      "\n",
      "Performance on test set:\n",
      "                   precision    recall  f1-score   support\n",
      "\n",
      "   69-я параллель       0.89      0.65      0.75       163\n",
      "           Бизнес       0.64      0.49      0.56       398\n",
      "      Бывший СССР       0.86      0.85      0.85      1362\n",
      "              Дом       0.88      0.84      0.86       681\n",
      "         Из жизни       0.81      0.78      0.79       981\n",
      "   Интернет и СМИ       0.82      0.80      0.81      1387\n",
      "             Крым       0.78      0.61      0.69       132\n",
      "    Культпросвет        0.71      0.33      0.45        61\n",
      "         Культура       0.87      0.91      0.89      1316\n",
      "              Мир       0.83      0.89      0.86      2885\n",
      "  Наука и техника       0.87      0.88      0.88      1129\n",
      "      Путешествия       0.87      0.82      0.84       644\n",
      "           Россия       0.78      0.84      0.81      3031\n",
      "Силовые структуры       0.79      0.74      0.76      1385\n",
      "            Спорт       0.96      0.97      0.97      2009\n",
      "         Ценности       0.94      0.91      0.92       896\n",
      "        Экономика       0.82      0.81      0.82      1537\n",
      "\n",
      "         accuracy                           0.84     19997\n",
      "        macro avg       0.83      0.77      0.79     19997\n",
      "     weighted avg       0.84      0.84      0.84     19997\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "vectorizer = CountVectorizer if best_params['vectorizer'] == 'CountVectorizer' else TfidfVectorizer\n",
    "\n",
    "if best_params['use_scaler']:\n",
    "    best_pipeline = Pipeline(\n",
    "        [\n",
    "            ('vectorizer', vectorizer(max_df=best_params['max_df'], min_df=best_params['min_df'])),\n",
    "            ('scaler', StandardScaler(with_mean=False)),\n",
    "            (\n",
    "                'classifier',\n",
    "                LogisticRegression(\n",
    "                    C=best_params['C'],\n",
    "                    random_state=42,\n",
    "                    max_iter=best_params['max_iter'],\n",
    "                    solver=best_params['solver'],\n",
    "                    penalty=best_params['penalty'],\n",
    "                ),\n",
    "            ),\n",
    "        ]\n",
    "    )\n",
    "else:\n",
    "    best_pipeline = Pipeline(\n",
    "        [\n",
    "            ('vectorizer', vectorizer(max_df=best_params['max_df'], min_df=best_params['min_df'])),\n",
    "            (\n",
    "                'classifier',\n",
    "                LogisticRegression(\n",
    "                    C=best_params['C'],\n",
    "                    random_state=42,\n",
    "                    max_iter=best_params['max_iter'],\n",
    "                    solver=best_params['solver'],\n",
    "                    penalty=best_params['penalty'],\n",
    "                ),\n",
    "            ),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "best_pipeline.fit(X_train, y_train)\n",
    "y_pred = best_pipeline.predict(X_test)\n",
    "\n",
    "print('Best hyperparameters:', best_params)\n",
    "print('\\nPerformance on test set:')\n",
    "print(classification_report(y_test, y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
